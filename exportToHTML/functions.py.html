<html>
<head>
<title>functions.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #0033b3;}
.s1 { color: #080808;}
.s2 { color: #1750eb;}
.s3 { color: #8c8c8c; font-style: italic;}
.s4 { color: #067d17;}
.s5 { color: #0037a6;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
functions.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">cross_val_score, KFold</span>
<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">BaseEstimator, ClassifierMixin</span>
<span class="s0">import </span><span class="s1">time</span>
<span class="s1">np.random.seed(</span><span class="s2">42</span><span class="s1">)</span>

<span class="s3">#### IQR method to remove outliers</span>

<span class="s0">def </span><span class="s1">remove_outliers(df, column):</span>
    <span class="s1">Q1 = df[column].quantile(</span><span class="s2">0.25</span><span class="s1">)</span>
    <span class="s1">Q3 = df[column].quantile(</span><span class="s2">0.75</span><span class="s1">)</span>
    <span class="s1">IQR = Q3 - Q1</span>
    <span class="s1">lower_bound = Q1 - </span><span class="s2">1.5 </span><span class="s1">* IQR</span>
    <span class="s1">upper_bound = Q3 + </span><span class="s2">1.5 </span><span class="s1">* IQR</span>
    <span class="s0">return </span><span class="s1">df[(df[column] &gt;= lower_bound) &amp; (df[column] &lt;= upper_bound)]</span>




<span class="s3">##### PERCEPTRON</span>
<span class="s0">class </span><span class="s1">Perceptron:</span>
    <span class="s3"># Initialize with maximum number of epochs and a vector of weights</span>
    <span class="s0">def </span><span class="s1">__init__(self, max_epochs=</span><span class="s2">1000</span><span class="s1">):</span>
        <span class="s1">self.max_epochs = max_epochs</span>
        <span class="s1">self.w = </span><span class="s0">None</span>

    <span class="s0">def </span><span class="s1">get_params(self, deep=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s3"># Return parameters as a dictionary</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s4">&quot;max_epochs&quot;</span><span class="s1">: self.max_epochs}</span>

    <span class="s0">def </span><span class="s1">set_params(self, **params):</span>
        <span class="s3"># Set model parameters</span>
        <span class="s0">for </span><span class="s1">key, value </span><span class="s0">in </span><span class="s1">params.items():</span>
            <span class="s1">setattr(self, key, value)</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">fit(self, X, y):</span>
        <span class="s3"># Updates weights vector until it reaches convergence or maximum number of epochs</span>
        <span class="s1">n_features = X.shape[</span><span class="s2">1</span><span class="s1">]</span>
        <span class="s1">self.w = np.zeros(n_features)</span>

        <span class="s3"># Print the parameters used</span>
        <span class="s1">print(</span><span class="s4">f&quot;Training with parameters: max_epochs=</span><span class="s5">{</span><span class="s1">self.max_epochs</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>

        <span class="s0">for </span><span class="s1">epoch </span><span class="s0">in </span><span class="s1">range(self.max_epochs):</span>
            <span class="s1">errors = </span><span class="s2">0</span>
            <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(X)):</span>
                <span class="s0">if </span><span class="s1">y[i] * (np.dot(X[i], self.w)) &lt;= </span><span class="s2">0</span><span class="s1">:</span>
                    <span class="s1">self.w += y[i] * X[i]</span>
                    <span class="s1">errors += </span><span class="s2">1</span>
            <span class="s0">if </span><span class="s1">errors == </span><span class="s2">0</span><span class="s1">:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Convergence reached at epoch number </span><span class="s5">{</span><span class="s1">epoch + </span><span class="s2">1</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>
                <span class="s0">break</span>
        <span class="s0">if </span><span class="s1">errors &gt; </span><span class="s2">0</span><span class="s1">:</span>
            <span class="s1">print(</span><span class="s4">f&quot;Max epochs (</span><span class="s5">{</span><span class="s1">self.max_epochs</span><span class="s5">}</span><span class="s4">) reached. NO convergence.&quot;</span><span class="s1">)</span>

        <span class="s3"># Calculate and print the final accuracy on the training set</span>
        <span class="s1">y_pred = self.predict(X)</span>
        <span class="s1">accuracy = np.mean(y == y_pred)</span>
        <span class="s1">print(</span><span class="s4">f&quot;Accuracy on training set: </span><span class="s5">{</span><span class="s1">accuracy</span><span class="s5">:</span><span class="s4">.4f</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>

        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">predict(self, X):</span>
        <span class="s3"># Returns an array of predictions</span>
        <span class="s0">return </span><span class="s1">np.sign(np.dot(X, self.w))</span>

    <span class="s0">def </span><span class="s1">score(self, X, y):</span>
        <span class="s1">predictions = self.predict(X)</span>
        <span class="s0">return </span><span class="s1">np.mean(predictions == y)</span>

<span class="s3">#### PEGASOS SVM</span>
<span class="s0">class </span><span class="s1">PegasosSVM(BaseEstimator, ClassifierMixin):</span>
    <span class="s3">#Initialise parameters (lamdbda and maximum number of iterations) and a weights vector</span>
    <span class="s0">def </span><span class="s1">__init__(self, lambda_param=</span><span class="s2">0.01</span><span class="s1">, max_iter=</span><span class="s2">1000</span><span class="s1">):</span>
        <span class="s1">self.lambda_param = lambda_param</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.w = </span><span class="s0">None</span>

    <span class="s0">def </span><span class="s1">get_params(self, deep=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s3"># Return parameters as a dictionary</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s4">&quot;lambda_param&quot;</span><span class="s1">: self.lambda_param, </span><span class="s4">&quot;max_iter&quot;</span><span class="s1">: self.max_iter}</span>

    <span class="s0">def </span><span class="s1">set_params(self, **params):</span>
        <span class="s3"># Set parameters of the model</span>
        <span class="s0">for </span><span class="s1">key, value </span><span class="s0">in </span><span class="s1">params.items():</span>
            <span class="s1">setattr(self, key, value)</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">fit(self, X, y):</span>
        <span class="s1">n_samples, n_features = X.shape</span>
        <span class="s1">self.w = np.zeros(n_features)</span>
        <span class="s1">cumulative_w = np.zeros(n_features)</span>

        <span class="s1">print(</span><span class="s4">f'Training with parameters: max_iter: </span><span class="s5">{</span><span class="s1">self.max_iter</span><span class="s5">}</span><span class="s4">, lambda: </span><span class="s5">{</span><span class="s1">self.lambda_param</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>

        <span class="s0">for </span><span class="s1">t </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">1</span><span class="s1">, self.max_iter + </span><span class="s2">1</span><span class="s1">):</span>
            <span class="s1">i = np.random.randint(</span><span class="s2">0</span><span class="s1">, n_samples)</span>
            <span class="s1">learning_rate = </span><span class="s2">1 </span><span class="s1">/ (self.lambda_param * t)</span>

            <span class="s0">if </span><span class="s1">y[i] * (np.dot(X[i], self.w)) &lt; </span><span class="s2">1</span><span class="s1">:</span>
                <span class="s1">self.w = (</span><span class="s2">1 </span><span class="s1">- learning_rate * self.lambda_param) * self.w + learning_rate * y[i] * X[i]</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">self.w = (</span><span class="s2">1 </span><span class="s1">- learning_rate * self.lambda_param) * self.w</span>

            <span class="s1">cumulative_w += self.w </span><span class="s3">#var made for mean</span>

        <span class="s3"># Final weights mean</span>
        <span class="s1">self.w = cumulative_w / self.max_iter</span>

        <span class="s3"># Calculate and print the final accuracy on the training set</span>
        <span class="s1">y_pred = self.predict(X)</span>
        <span class="s1">accuracy = np.mean(y == y_pred)</span>
        <span class="s1">print(</span><span class="s4">f&quot;Accuracy on training set: </span><span class="s5">{</span><span class="s1">accuracy</span><span class="s5">:</span><span class="s4">.4f</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>

        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">predict(self, X):</span>
        <span class="s3"># Returns an array of predictions</span>
        <span class="s0">return </span><span class="s1">np.sign(np.dot(X, self.w))</span>

    <span class="s0">def </span><span class="s1">score(self, X, y):</span>
        <span class="s1">predictions = self.predict(X)</span>
        <span class="s0">return </span><span class="s1">np.mean(predictions == y)</span>



<span class="s3">#### LOGISTIC REGRESSION</span>

<span class="s0">class </span><span class="s1">PegasosLogisticRegression(BaseEstimator, ClassifierMixin):</span>
    <span class="s0">def </span><span class="s1">__init__(self, lambda_param=</span><span class="s2">0.01</span><span class="s1">, max_iter=</span><span class="s2">1000</span><span class="s1">):</span>
        <span class="s3"># Initialise parameters (lambda and maximum number of iterations) and a weights vector</span>
        <span class="s1">self.lambda_param = lambda_param</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.w = </span><span class="s0">None</span>

    <span class="s0">def </span><span class="s1">get_params(self, deep=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s3"># Return parameters as a dictionary</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s4">&quot;lambda_param&quot;</span><span class="s1">: self.lambda_param, </span><span class="s4">&quot;max_iter&quot;</span><span class="s1">: self.max_iter}</span>

    <span class="s0">def </span><span class="s1">set_params(self, **params):</span>
        <span class="s3"># Imposta i parametri del modello</span>
        <span class="s0">for </span><span class="s1">key, value </span><span class="s0">in </span><span class="s1">params.items():</span>
            <span class="s1">setattr(self, key, value)</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">sigmoid(self, z):</span>
        <span class="s3"># Maps linear combination (z) to probability (0, 1)</span>
        <span class="s1">z = np.clip(z, -</span><span class="s2">700</span><span class="s1">, </span><span class="s2">700</span><span class="s1">)  </span><span class="s3"># Limiting Z to avoid overflow</span>
        <span class="s0">return </span><span class="s2">1 </span><span class="s1">/ (</span><span class="s2">1 </span><span class="s1">+ np.exp(-z))</span>

    <span class="s0">def </span><span class="s1">fit(self, X, y):</span>

        <span class="s1">n_samples, n_features = X.shape</span>
        <span class="s1">self.w = np.zeros(n_features)  </span><span class="s3"># Weight initialization</span>
        <span class="s1">cumulative_w = np.zeros(n_features)  </span><span class="s3"># Mean purposes</span>

        <span class="s1">print(</span><span class="s4">f'Training with parameters: max_iter: </span><span class="s5">{</span><span class="s1">self.max_iter</span><span class="s5">}</span><span class="s4">, lambda: </span><span class="s5">{</span><span class="s1">self.lambda_param</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>

        <span class="s0">for </span><span class="s1">t </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">1</span><span class="s1">, self.max_iter + </span><span class="s2">1</span><span class="s1">):</span>
            <span class="s3"># Random selection of an index</span>
            <span class="s1">i = np.random.randint(</span><span class="s2">0</span><span class="s1">, n_samples)</span>
            <span class="s1">learning_rate = </span><span class="s2">1 </span><span class="s1">/ (self.lambda_param * t)</span>

            <span class="s3"># Computing probability with sigmoid function</span>
            <span class="s1">margin = y[i] * (np.dot(X[i], self.w))</span>
            <span class="s1">prob = self.sigmoid(margin)</span>

            <span class="s3"># Updating weights</span>
            <span class="s1">self.w = (</span><span class="s2">1 </span><span class="s1">- learning_rate * self.lambda_param) * self.w + learning_rate * y[i] * X[i] * (</span><span class="s2">1 </span><span class="s1">- prob)</span>

            <span class="s3"># For mean purposes</span>
            <span class="s1">cumulative_w += self.w</span>

        <span class="s3"># Average of final weights</span>
        <span class="s1">self.w = cumulative_w / self.max_iter</span>

        <span class="s3"># Calculate and print the final accuracy on the training set</span>
        <span class="s1">y_pred = self.predict(X)</span>
        <span class="s1">accuracy = np.mean(y == y_pred)</span>
        <span class="s1">print(</span><span class="s4">f&quot;Accuracy on training set: </span><span class="s5">{</span><span class="s1">accuracy</span><span class="s5">:</span><span class="s4">.4f</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>

        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">predict_proba(self, X):</span>
        <span class="s1">linear_output = np.dot(X, self.w)</span>
        <span class="s0">return </span><span class="s1">self.sigmoid(linear_output)</span>

    <span class="s0">def </span><span class="s1">predict(self, X):</span>
        <span class="s3"># Returns an array of predictions</span>
        <span class="s1">prob = self.predict_proba(X)</span>
        <span class="s0">return </span><span class="s1">np.where(prob &gt;= </span><span class="s2">0.5</span><span class="s1">, </span><span class="s2">1</span><span class="s1">, -</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">score(self, X, y):</span>
        <span class="s1">predictions = self.predict(X)</span>
        <span class="s0">return </span><span class="s1">np.mean(predictions == y)</span>




<span class="s3">#### POLYNOMIAL FEATURE EXPANSION</span>

<span class="s0">def </span><span class="s1">polynomial_feature_expansion(X):</span>
    <span class="s1">n_samples, n_features = X.shape</span>
    <span class="s1">new_features = []</span>

    <span class="s1">new_features.append(X)  </span><span class="s3"># add original features</span>

    <span class="s3"># Add quadratic features</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_features):</span>
        <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(i, n_features):</span>
            <span class="s1">new_features.append(X[:, i] * X[:, j])</span>

    <span class="s3"># Combine all new features in 1 array</span>
    <span class="s1">X_poly = np.column_stack(new_features)</span>

    <span class="s0">return </span><span class="s1">X_poly</span>




<span class="s3">#### KERNELISED PERCEPTRON (GAUSSIAN AND POLYNOMIAL)</span>

<span class="s3"># define gaussian and polynomial kernel functions</span>

<span class="s0">def </span><span class="s1">gaussian_kernel(X, Y, gamma=</span><span class="s2">0.1</span><span class="s1">):</span>
    <span class="s3"># Computing Gaussian Kernel</span>
    <span class="s0">return </span><span class="s1">np.exp(-np.linalg.norm(X - Y) ** </span><span class="s2">2 </span><span class="s1">/ (</span><span class="s2">2 </span><span class="s1">* gamma))</span>

<span class="s0">def </span><span class="s1">polynomial_kernel(X, Y, degree=</span><span class="s2">3</span><span class="s1">):</span>
    <span class="s3"># Computing Polynomial Kernel</span>
    <span class="s0">return </span><span class="s1">(</span><span class="s2">1 </span><span class="s1">+ np.dot(X, Y.T)) ** degree</span>

<span class="s3"># Main algorithm</span>

<span class="s0">class </span><span class="s1">KernelPerceptron:</span>
    <span class="s0">def </span><span class="s1">__init__(self, max_epochs=</span><span class="s2">1000</span><span class="s1">, kernel=polynomial_kernel, **kernel_params):</span>
        <span class="s1">self.max_epochs = max_epochs</span>
        <span class="s1">self.kernel = kernel</span>
        <span class="s1">self.kernel_params = kernel_params</span>
        <span class="s1">self.support_vectors = []</span>
        <span class="s1">self.support_labels = []</span>

    <span class="s0">def </span><span class="s1">get_params(self, deep=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s3"># Return parameters as a dictionary</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s4">&quot;max_epochs&quot;</span><span class="s1">: self.max_epochs, </span><span class="s4">&quot;kernel&quot;</span><span class="s1">: self.kernel, **self.kernel_params}</span>

    <span class="s0">def </span><span class="s1">set_params(self, **params):</span>
        <span class="s3"># Imposta i parametri del modello</span>
        <span class="s0">for </span><span class="s1">key, value </span><span class="s0">in </span><span class="s1">params.items():</span>
            <span class="s1">setattr(self, key, value)</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">fit(self, X, y):</span>
        <span class="s1">n_samples = len(y)</span>

        <span class="s1">print(</span><span class="s4">f'Training with parameters: max_epochs: </span><span class="s5">{</span><span class="s1">self.max_epochs</span><span class="s5">}</span><span class="s4">, kernel: </span><span class="s5">{</span><span class="s1">self.kernel</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>

        <span class="s0">for </span><span class="s1">epoch </span><span class="s0">in </span><span class="s1">range(self.max_epochs):</span>
            <span class="s1">errors = </span><span class="s2">0</span>
            <span class="s1">start_time = time.time()</span>
            <span class="s0">for </span><span class="s1">t </span><span class="s0">in </span><span class="s1">range(n_samples):</span>
                <span class="s3"># Compute prediction only using support vectors</span>
                <span class="s1">kernel_eval = sum(self.support_labels[i] * self.kernel(self.support_vectors[i], X[t], **self.kernel_params)</span>
                                      <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(self.support_vectors)))</span>
                <span class="s1">y_pred = np.sign(kernel_eval)</span>

                <span class="s3"># If prediction is wrong, add the example to support vectors</span>
                <span class="s0">if </span><span class="s1">y_pred != y[t]:</span>
                    <span class="s1">self.support_vectors.append(X[t])</span>
                    <span class="s1">self.support_labels.append(y[t])</span>
                    <span class="s1">errors +=</span><span class="s2">1</span>
            <span class="s0">if </span><span class="s1">errors == </span><span class="s2">0</span><span class="s1">:</span>
                <span class="s1">print(</span><span class="s4">f&quot;Convergence reached at epoch </span><span class="s5">{</span><span class="s1">epoch + </span><span class="s2">1</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>
                <span class="s1">end_time = time.time()</span>
                <span class="s1">total_time = end_time - start_time</span>
                <span class="s1">print(</span><span class="s4">f&quot;Converged after </span><span class="s5">{</span><span class="s1">epoch + </span><span class="s2">1</span><span class="s5">} </span><span class="s4">epochs in </span><span class="s5">{</span><span class="s1">total_time</span><span class="s5">:</span><span class="s4">.2f</span><span class="s5">} </span><span class="s4">seconds&quot;</span><span class="s1">)</span>
                <span class="s0">break</span>
        <span class="s0">if </span><span class="s1">errors &gt; </span><span class="s2">0</span><span class="s1">:</span>
            <span class="s1">end_time = time.time()</span>
            <span class="s1">total_time = end_time - start_time</span>
            <span class="s1">print(</span><span class="s4">f&quot;Did not converge after </span><span class="s5">{</span><span class="s1">self.max_epochs</span><span class="s5">} </span><span class="s4">epochs. Total time: </span><span class="s5">{</span><span class="s1">total_time</span><span class="s5">:</span><span class="s4">.2f</span><span class="s5">} </span><span class="s4">seconds&quot;</span><span class="s1">)</span>

         <span class="s3"># Calculate and print the final accuracy on the training set</span>
        <span class="s1">y_pred = self.predict(X)</span>
        <span class="s1">accuracy = np.mean(y == y_pred)</span>
        <span class="s1">print(</span><span class="s4">f&quot;Accuracy on training set: </span><span class="s5">{</span><span class="s1">accuracy</span><span class="s5">:</span><span class="s4">.4f</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">predict(self, X):</span>
        <span class="s0">if </span><span class="s1">len(self.support_vectors) == </span><span class="s2">0</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">np.zeros(X.shape[</span><span class="s2">0</span><span class="s1">])</span>
        <span class="s1">kernel_sum = np.array([sum(self.support_labels[i] * self.kernel(self.support_vectors[i], x, **self.kernel_params)</span>
                                   <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(self.support_vectors))) </span><span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">X])</span>
        <span class="s0">return </span><span class="s1">np.sign(kernel_sum)</span>

    <span class="s0">def </span><span class="s1">score(self, X, y):</span>
        <span class="s1">predictions = self.predict(X)</span>
        <span class="s0">return </span><span class="s1">np.mean(predictions == y)</span>




<span class="s3">#### KERNELISED PEGASOS WITH GAUSSIAN AND POLYNOMIAL KERNELS SVM</span>

<span class="s0">class </span><span class="s1">KernelPegasosSVM:</span>
    <span class="s0">def </span><span class="s1">__init__(self, lambda_param=</span><span class="s2">0.01</span><span class="s1">, max_iter=</span><span class="s2">1000</span><span class="s1">, kernel=gaussian_kernel, **kernel_params):</span>
        <span class="s1">self.lambda_param = lambda_param</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.kernel = kernel</span>
        <span class="s1">self.kernel_params = kernel_params</span>
        <span class="s1">self.alpha = </span><span class="s0">None</span>
        <span class="s1">self.support_vectors = </span><span class="s0">None</span>
        <span class="s1">self.support_labels = </span><span class="s0">None</span>

    <span class="s0">def </span><span class="s1">get_params(self, deep=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s3"># Restituisce i parametri del modello</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s4">&quot;lambda_param&quot;</span><span class="s1">: self.lambda_param, </span><span class="s4">&quot;max_iter&quot;</span><span class="s1">: self.max_iter, </span><span class="s4">&quot;kernel&quot;</span><span class="s1">: self.kernel,</span>
                <span class="s1">**self.kernel_params}</span>

    <span class="s0">def </span><span class="s1">set_params(self, **params):</span>
        <span class="s3"># Imposta i parametri del modello</span>
        <span class="s0">for </span><span class="s1">key, value </span><span class="s0">in </span><span class="s1">params.items():</span>
            <span class="s1">setattr(self, key, value)</span>
        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">fit(self, X, y):</span>
        <span class="s1">n_samples = len(X)</span>
        <span class="s1">self.alpha = np.zeros(n_samples)</span>
        <span class="s1">self.support_vectors = X</span>
        <span class="s1">self.support_labels = y</span>



        <span class="s0">for </span><span class="s1">t </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">1</span><span class="s1">, self.max_iter + </span><span class="s2">1</span><span class="s1">):</span>
            <span class="s1">i_t = np.random.randint(</span><span class="s2">0</span><span class="s1">, n_samples)</span>
            <span class="s1">sum_term = sum(self.alpha[j] * y[j] * self.kernel(X[i_t], X[j], **self.kernel_params) </span><span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(n_samples))</span>

            <span class="s0">if </span><span class="s1">y[i_t] * (</span><span class="s2">1 </span><span class="s1">/ (self.lambda_param * t)) * sum_term &lt; </span><span class="s2">1</span><span class="s1">:</span>
                <span class="s1">self.alpha[i_t] += </span><span class="s2">1</span>

        <span class="s3"># Calculate and print the final accuracy on the training set</span>
        <span class="s1">y_pred = self.predict(X)</span>
        <span class="s1">accuracy = np.mean(y == y_pred)</span>
        <span class="s1">print(</span><span class="s4">f&quot;Accuracy on training set: </span><span class="s5">{</span><span class="s1">accuracy</span><span class="s5">:</span><span class="s4">.4f</span><span class="s5">}</span><span class="s4">&quot;</span><span class="s1">)</span>

        <span class="s0">return </span><span class="s1">self</span>

    <span class="s0">def </span><span class="s1">predict(self, X):</span>
        <span class="s1">y_pred = []</span>
        <span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">X:</span>
            <span class="s1">prediction = np.sign(</span>
                <span class="s1">sum(self.alpha[j] * self.support_labels[j] * self.kernel(self.support_vectors[j], x)</span>
                    <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(len(self.support_vectors)))</span>
            <span class="s1">)</span>
            <span class="s1">y_pred.append(prediction)</span>
        <span class="s0">return </span><span class="s1">np.array(y_pred)</span>

    <span class="s0">def </span><span class="s1">score(self, X, y):</span>
        <span class="s1">predictions = self.predict(X)</span>
        <span class="s1">accuracy = np.mean(predictions == y)</span>
        <span class="s0">return </span><span class="s1">accuracy</span>



<span class="s3">#### COMPARE ORIGINAL AND EXPANDED WEIGHTS</span>

<span class="s0">def </span><span class="s1">compare_weights(model, feature_names, feature_names_poly, X_train, X_train_poly, y_train):</span>
    <span class="s3"># Training model on original features</span>
    <span class="s1">model.fit(X_train, y_train)</span>
    <span class="s1">original_weights = model.w</span>

    <span class="s3"># Training model on polynomial features</span>
    <span class="s1">model.fit(X_train_poly, y_train)</span>
    <span class="s1">poly_weights = model.w</span>

    <span class="s3"># Create a dataframe to visualise original weights</span>
    <span class="s1">original_weights_df = pd.DataFrame({</span><span class="s4">'Feature'</span><span class="s1">: feature_names, </span><span class="s4">'Weight'</span><span class="s1">: original_weights})</span>
    <span class="s1">original_weights_df[</span><span class="s4">'Abs_Weight'</span><span class="s1">] = original_weights_df[</span><span class="s4">'Weight'</span><span class="s1">].abs()</span>
    <span class="s1">original_weights_df = original_weights_df.sort_values(by=</span><span class="s4">'Abs_Weight'</span><span class="s1">, ascending=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">print(</span><span class="s4">&quot;Original features weights:&quot;</span><span class="s1">)</span>
    <span class="s1">print(original_weights_df)</span>

    <span class="s3"># Create a dataframe to visualise polynomial weighTs</span>
    <span class="s1">poly_weights_df = pd.DataFrame({</span><span class="s4">'Feature'</span><span class="s1">: feature_names_poly, </span><span class="s4">'Weight'</span><span class="s1">: poly_weights})</span>
    <span class="s1">poly_weights_df[</span><span class="s4">'Abs_Weight'</span><span class="s1">] = poly_weights_df[</span><span class="s4">'Weight'</span><span class="s1">].abs()</span>
    <span class="s1">poly_weights_df = poly_weights_df.sort_values(by=</span><span class="s4">'Abs_Weight'</span><span class="s1">, ascending=</span><span class="s0">False</span><span class="s1">)</span>

    <span class="s1">print(</span><span class="s4">&quot;Features weights after polynomial expansion:&quot;</span><span class="s1">)</span>
    <span class="s1">print(poly_weights_df)</span>




<span class="s3">#### CROSS-VALIDATION ON SVM AND LOG_REG (done automatically with original and expanded features)</span>

<span class="s0">def </span><span class="s1">cross_val_model(model, param_grid, X_train, y_train, X_train_poly):</span>
    <span class="s1">kf = KFold(n_splits=</span><span class="s2">5</span><span class="s1">, shuffle=</span><span class="s0">True</span><span class="s1">, random_state=</span><span class="s2">42</span><span class="s1">)</span>

    <span class="s1">best_score = -np.inf</span>
    <span class="s1">best_params = </span><span class="s0">None</span>
    <span class="s1">best_feature_type = </span><span class="s0">None</span>

    <span class="s0">for </span><span class="s1">lambda_param </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'lambda_param'</span><span class="s1">]:</span>
        <span class="s0">for </span><span class="s1">max_iter </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'max_iter'</span><span class="s1">]:</span>
            <span class="s1">model.set_params(lambda_param=lambda_param, max_iter=max_iter)</span>

            <span class="s3"># Cross-validation on original features</span>
            <span class="s1">scores = cross_val_score(model, X_train, y_train, cv=kf)</span>
            <span class="s1">mean_score = np.mean(scores)</span>
            <span class="s1">print(</span><span class="s4">'Accuracy with lambda ('</span><span class="s1">, lambda_param, </span><span class="s4">') and max_iter ('</span><span class="s1">, max_iter, </span><span class="s4">') is '</span><span class="s1">, mean_score)</span>

            <span class="s3"># Update best parameters</span>
            <span class="s0">if </span><span class="s1">mean_score &gt; best_score:</span>
                <span class="s1">best_score = mean_score</span>
                <span class="s1">best_params = {</span><span class="s4">'lambda_param'</span><span class="s1">: lambda_param, </span><span class="s4">'max_iter'</span><span class="s1">: max_iter}</span>
                <span class="s1">best_feature_type = </span><span class="s4">'original features'</span>

            <span class="s3"># Cross-validation on polynomial features</span>
            <span class="s1">scores_poly = cross_val_score(model, X_train_poly, y_train, cv=kf)</span>
            <span class="s1">mean_score_poly = np.mean(scores_poly)</span>
            <span class="s1">print(</span><span class="s4">'Accuracy with max_iter ('</span><span class="s1">, max_iter, </span><span class="s4">') and lambda ('</span><span class="s1">,lambda_param,</span><span class="s4">') is'</span><span class="s1">, mean_score_poly)</span>

            <span class="s0">if </span><span class="s1">mean_score_poly &gt; best_score:</span>
                <span class="s1">best_score = mean_score_poly</span>
                <span class="s1">best_params = {</span><span class="s4">'lambda_param'</span><span class="s1">: lambda_param, </span><span class="s4">'max_iter'</span><span class="s1">: max_iter}</span>
                <span class="s1">best_feature_type = </span><span class="s4">'polynomial features'</span>

    <span class="s1">print(</span><span class="s4">f'Best Parameters for </span><span class="s5">{</span><span class="s1">model</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s4">f'Best Cross-Validation Accuracy for </span><span class="s5">{</span><span class="s1">model</span><span class="s5">}</span><span class="s4">: </span><span class="s5">{</span><span class="s1">best_score</span><span class="s5">:</span><span class="s4">.2f</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s4">f'Model with </span><span class="s5">{</span><span class="s1">best_feature_type</span><span class="s5">} </span><span class="s4">is better.'</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">best_params</span>




<span class="s3">#### CROSS-VALIDATION ON PERCEPTRON (distinct function because only one parameter)</span>

<span class="s0">def </span><span class="s1">cross_val_model_perceptron(model, max_iter_grid, X_train, y_train, X_train_poly):</span>
    <span class="s1">kf = KFold(n_splits=</span><span class="s2">5</span><span class="s1">, shuffle=</span><span class="s0">True</span><span class="s1">, random_state=</span><span class="s2">42</span><span class="s1">)</span>

    <span class="s1">best_score = -np.inf</span>
    <span class="s1">best_params = </span><span class="s0">None</span>
    <span class="s1">best_feature_type = </span><span class="s0">None</span>

    <span class="s0">for </span><span class="s1">max_iter </span><span class="s0">in </span><span class="s1">max_iter_grid:</span>
        <span class="s1">model.set_params(max_epochs=max_iter)</span>

        <span class="s3"># Cross-validation on original features</span>
        <span class="s1">scores = cross_val_score(model, X_train, y_train, cv=kf)</span>
        <span class="s1">mean_score = np.mean(scores)</span>
        <span class="s1">print(</span><span class="s4">'Accuracy with max_iter ('</span><span class="s1">, max_iter, </span><span class="s4">') is '</span><span class="s1">, mean_score)</span>

        <span class="s3"># Updating best parameters</span>
        <span class="s0">if </span><span class="s1">mean_score &gt; best_score:</span>
            <span class="s1">best_score = mean_score</span>
            <span class="s1">best_params = {</span><span class="s4">'max_iter'</span><span class="s1">: max_iter}</span>
            <span class="s1">best_feature_type = </span><span class="s4">'original features'</span>

        <span class="s3"># Cross-validation on polynomial features</span>
        <span class="s1">scores_poly = cross_val_score(model, X_train_poly, y_train, cv=kf)</span>
        <span class="s1">mean_score_poly = np.mean(scores_poly)</span>
        <span class="s1">print(</span><span class="s4">'Accuracy with max_iter ('</span><span class="s1">, max_iter ,</span><span class="s4">') is'</span><span class="s1">, mean_score_poly)</span>

        <span class="s0">if </span><span class="s1">mean_score_poly &gt; best_score:</span>
            <span class="s1">best_score = mean_score_poly</span>
            <span class="s1">best_params = {</span><span class="s4">'max_iter'</span><span class="s1">: max_iter}</span>
            <span class="s1">best_feature_type = </span><span class="s4">'polynomial features'</span>

    <span class="s1">print(</span><span class="s4">f'Best Parameters for Perceptron: </span><span class="s5">{</span><span class="s1">best_params</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s4">f'Best Cross-Validation Accuracy: </span><span class="s5">{</span><span class="s1">best_score</span><span class="s5">:</span><span class="s4">.2f</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s4">f'Model with </span><span class="s5">{</span><span class="s1">best_feature_type</span><span class="s5">} </span><span class="s4">is better.'</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">best_params</span>




<span class="s3">#### CROSS-VALIDATION ON KERNELISED PERCEPTRON (GAUSSIAN OR POLYNOMIAL)</span>
<span class="s0">def </span><span class="s1">cross_val_model_kernel(model, param_grid, X_train, y_train, kernel_type=</span><span class="s4">'gaussian'</span><span class="s1">):</span>
    <span class="s1">kf = KFold(n_splits=</span><span class="s2">5</span><span class="s1">, shuffle=</span><span class="s0">True</span><span class="s1">, random_state=</span><span class="s2">42</span><span class="s1">)</span>

    <span class="s1">best_score = -np.inf</span>
    <span class="s1">best_params = </span><span class="s0">None</span>

    <span class="s0">for </span><span class="s1">max_epochs </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'max_epochs'</span><span class="s1">]:</span>
        <span class="s0">if </span><span class="s1">kernel_type == </span><span class="s4">'gaussian'</span><span class="s1">:</span>
            <span class="s0">for </span><span class="s1">gamma </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'gamma'</span><span class="s1">]:</span>
                <span class="s1">model.set_params(max_epochs=max_epochs, gamma=gamma)</span>
                <span class="s1">scores = cross_val_score(model, X_train, y_train, cv=kf)</span>
                <span class="s1">mean_score = np.mean(scores)</span>
                <span class="s0">if </span><span class="s1">mean_score &gt; best_score:</span>
                    <span class="s1">best_score = mean_score</span>
                    <span class="s1">best_params = {</span><span class="s4">'gamma'</span><span class="s1">: gamma, </span><span class="s4">'max_epochs'</span><span class="s1">: max_epochs}</span>

        <span class="s0">elif </span><span class="s1">kernel_type == </span><span class="s4">'polynomial'</span><span class="s1">:</span>
            <span class="s0">for </span><span class="s1">degree </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'degree'</span><span class="s1">]:</span>
                <span class="s1">model.set_params(max_epochs=max_epochs, degree=degree)</span>
                <span class="s1">scores = cross_val_score(model, X_train, y_train, cv=kf)</span>
                <span class="s1">mean_score = np.mean(scores)</span>

                <span class="s0">if </span><span class="s1">mean_score &gt; best_score:</span>
                    <span class="s1">best_score = mean_score</span>
                    <span class="s1">best_params = {</span><span class="s4">'degree'</span><span class="s1">: degree, </span><span class="s4">'max_epochs'</span><span class="s1">: max_epochs}</span>

    <span class="s1">print(</span><span class="s4">f'Best Parameters for Perceptron (</span><span class="s5">{</span><span class="s1">kernel_type</span><span class="s5">} </span><span class="s4">kernel): </span><span class="s5">{</span><span class="s1">best_params</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s4">f'Best Cross-Validation Accuracy for </span><span class="s5">{</span><span class="s1">model</span><span class="s5">}</span><span class="s4">: </span><span class="s5">{</span><span class="s1">best_score</span><span class="s5">:</span><span class="s4">.2f</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">best_params</span>



<span class="s3">#### CROSS-VALIDATION ON KERNELISED PEGASOS SVM (GAUSSIAN OR POLYNOMIAL)</span>
<span class="s0">def </span><span class="s1">cross_val_model_kernelSVM(model, param_grid, X_train, y_train, kernel_type=</span><span class="s4">'gaussian'</span><span class="s1">):</span>
    <span class="s1">kf = KFold(n_splits=</span><span class="s2">5</span><span class="s1">, shuffle=</span><span class="s0">True</span><span class="s1">, random_state=</span><span class="s2">42</span><span class="s1">)</span>

    <span class="s1">best_score = -np.inf</span>
    <span class="s1">best_params = </span><span class="s0">None</span>

    <span class="s0">if </span><span class="s1">kernel_type == </span><span class="s4">'gaussian'</span><span class="s1">:</span>
        <span class="s0">for </span><span class="s1">gamma </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'gamma'</span><span class="s1">]:</span>
            <span class="s0">for </span><span class="s1">lambda_param </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'lambda_param'</span><span class="s1">]:</span>
                <span class="s1">model.set_params(lambda_param=lambda_param, gamma=gamma)</span>
                <span class="s1">scores = cross_val_score(model, X_train, y_train, cv=kf)</span>
                <span class="s1">mean_score = np.mean(scores)</span>
                <span class="s0">if </span><span class="s1">mean_score &gt; best_score:</span>
                    <span class="s1">best_score = mean_score</span>
                    <span class="s1">best_params = {</span><span class="s4">'gamma'</span><span class="s1">: gamma, </span><span class="s4">'lambda_param'</span><span class="s1">: lambda_param}</span>

    <span class="s0">elif </span><span class="s1">kernel_type == </span><span class="s4">'polynomial'</span><span class="s1">:</span>
        <span class="s0">for </span><span class="s1">degree </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'degree'</span><span class="s1">]:</span>
            <span class="s0">for </span><span class="s1">lambda_param </span><span class="s0">in </span><span class="s1">param_grid[</span><span class="s4">'lambda_param'</span><span class="s1">]:</span>
                <span class="s1">model.set_params(degree=degree, lambda_param=lambda_param)</span>
                <span class="s1">scores = cross_val_score(model, X_train, y_train, cv=kf)</span>
                <span class="s1">mean_score = np.mean(scores)</span>
                <span class="s0">if </span><span class="s1">mean_score &gt; best_score:</span>
                    <span class="s1">best_score = mean_score</span>
                    <span class="s1">best_params = {</span><span class="s4">'degree'</span><span class="s1">: degree, </span><span class="s4">'lambda_param'</span><span class="s1">: lambda_param}</span>

    <span class="s1">print(</span><span class="s4">f'Best Parameters for Pegasos SVM (</span><span class="s5">{</span><span class="s1">kernel_type</span><span class="s5">} </span><span class="s4">kernel): </span><span class="s5">{</span><span class="s1">best_params</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s4">f'Best Cross-Validation Accuracy for Pegasos SVM: </span><span class="s5">{</span><span class="s1">best_score</span><span class="s5">:</span><span class="s4">.2f</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">best_params</span>




<span class="s3">#### APPLYING BEST MODEL TO TEST SET AND COMPUTING ACCURACY</span>

<span class="s0">def </span><span class="s1">evaluate_model_on_test(model, X_train_poly, y_train, X_test_poly, y_test):</span>
    <span class="s3"># Training model with the best parameters on whole training set</span>
    <span class="s1">model.fit(X_train_poly, y_train)</span>

    <span class="s3"># Predict labels of test set</span>
    <span class="s1">y_pred_test = model.predict(X_test_poly)</span>

    <span class="s3"># Compute accuracy</span>
    <span class="s1">accuracy = np.mean(y_test == y_pred_test)</span>
    <span class="s1">print(</span><span class="s4">f'Accuracy on test set: </span><span class="s5">{</span><span class="s1">accuracy</span><span class="s5">:</span><span class="s4">.2f</span><span class="s5">}</span><span class="s4">'</span><span class="s1">)</span></pre>
</body>
</html>